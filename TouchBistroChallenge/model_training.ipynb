{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "\n",
    "from category_encoders import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
    "from optuna_integration import XGBoostPruningCallback\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "COLORS = sns.color_palette()\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "pd.options.mode.copy_on_write = True\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Seaborn version: {sns.__version__}\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "print(f\"CuPy version: {cp.__version__}\")\n",
    "print(f\"Optuna version: {optuna.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bills = pd.read_csv('./bills.csv', usecols=['business_date', 'sales_revenue_with_tax', 'venue_xref_id'])\n",
    "\n",
    "bills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venues = pd.read_csv('./venues.csv')\n",
    "venues.drop(columns=['start_of_day_offset'], inplace=True)\n",
    "\n",
    "venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge grouped_bills and venues along the venue_xref_id axis\n",
    "df = pd.merge(left=bills.groupby(['business_date', 'venue_xref_id'])['sales_revenue_with_tax'].agg(['sum', 'count']).reset_index(),\n",
    "              right=venues,\n",
    "              how='outer',\n",
    "              on='venue_xref_id',\n",
    "              validate='m:1')\n",
    "df['business_date'] = pd.to_datetime(df['business_date'])\n",
    "df.rename(columns={'sum': 'sales_revenue_with_tax', 'count': 'num_bills'}, inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_bills'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.displot(data=df['num_bills'].loc[df['num_bills'] <= 500], \n",
    "                kde=True, height=5, aspect=2)\n",
    "g.set_xlabels('Number of Daily Venue Bills')\n",
    "g.set_titles('Distribution of Daily Venue Bills')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.displot(data=df['num_bills'].loc[df['num_bills'] > 500], \n",
    "                kde=True, height=5, aspect=2)\n",
    "g.set_xlabels('Number of Daily Venue Bills')\n",
    "g.set_titles('Distribution of Daily Venue Bills')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create new features for the data.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Add new features\n",
    "    df['day_of_week'] = df['business_date'].dt.day_of_week\n",
    "    df['day_of_year'] = df['business_date'].dt.day_of_year\n",
    "    df['month'] = df['business_date'].dt.month\n",
    "    df['quarter'] = df['business_date'].dt.quarter\n",
    "\n",
    "    return df\n",
    "\n",
    "def encode_categorical(df: pd.DataFrame, encoder: OrdinalEncoder, features: list) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # Encode categorical data\n",
    "    df = encoder.transform(df)\n",
    "\n",
    "    # Specify categorical data type for XGBoost\n",
    "    df[features] = df[features].astype('category')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = ['venue_xref_id', 'concept', 'city', 'country', \n",
    "            'day_of_week', 'day_of_year', 'month', 'quarter']\n",
    "TARGET = 'sales_revenue_with_tax'\n",
    "\n",
    "df = create_features(df)\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder(cols=FEATURES[:4])\n",
    "ordinal_encoder.fit(df[FEATURES])\n",
    "\n",
    "df[FEATURES] = encode_categorical(df[FEATURES], ordinal_encoder, FEATURES)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save encoder for loading the encoder later and transforming new data\n",
    "with open('encoder.obj', 'wb') as f:\n",
    "    pickle.dump(ordinal_encoder, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2, shuffle=True, random_state=RANDOM_SEED)\n",
    "test, valid = train_test_split(test, test_size=0.5, shuffle=True, random_state=RANDOM_SEED)\n",
    "train.shape, valid.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[FEATURES]\n",
    "y_train = train[TARGET]\n",
    "\n",
    "X_valid = valid[FEATURES]\n",
    "y_valid = valid[TARGET]\n",
    "\n",
    "X_test = test[FEATURES]\n",
    "y_test = test[TARGET]\n",
    "\n",
    "X_train.shape, y_train.shape, X_valid.shape, y_valid.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning Tree Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.abs(y_true - y_pred).sum() / np.abs(y_true).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = weighted_absolute_percentage_error\n",
    "\n",
    "base_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': metric,\n",
    "    'learning_rate': 0.3,\n",
    "    'enable_categorical': True,\n",
    "    'booster': 'gbtree',\n",
    "    'random_state': RANDOM_SEED,\n",
    "    'device': 'cuda',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define objective function for Optuna\n",
    "def objective_fn(trial: optuna.trial.Trial) -> float:\n",
    "    params = {\n",
    "        'tree_method': trial.suggest_categorical('tree_method', ['approx', 'hist']),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 250),\n",
    "        'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n",
    "        'colsample_bynode': trial.suggest_float('colsample_bynode', 0.1, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 25, log=True),\n",
    "    }\n",
    "\n",
    "    params.update(base_params)\n",
    "    pruning_callback = XGBoostPruningCallback(trial, f'validation_1-{metric.__name__}')\n",
    "\n",
    "    regressor = xgb.XGBRegressor(n_estimators=10000,\n",
    "                                 early_stopping_rounds=50,\n",
    "                                 callbacks=[pruning_callback], \n",
    "                                 **params)\n",
    "    \n",
    "    regressor.fit(X_train, y_train,\n",
    "                  eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "                  verbose=False)\n",
    "\n",
    "    trial.set_user_attr('best_iteration', regressor.best_iteration)\n",
    "    return regressor.best_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = optuna.samplers.TPESampler(seed=RANDOM_SEED)\n",
    "study = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "\n",
    "tic = time.time()\n",
    "while time.time() - tic < 300:\n",
    "    study.optimize(objective_fn, n_trials=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Stage 1 ==============================')\n",
    "print(f'best score = {study.best_trial.value:.2%}')\n",
    "print('boosting params ---------------------------')\n",
    "print(f'fixed learning rate: {base_params['learning_rate']}')\n",
    "print(f'best boosting round: {study.best_trial.user_attrs['best_iteration']}')\n",
    "print('best tree params --------------------------')\n",
    "for parameter, value in study.best_trial.params.items():\n",
    "    print(parameter, ':', value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning Booster Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params.update(base_params)\n",
    "params.update(study.best_trial.params)\n",
    "params['learning_rate'] = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = xgb.XGBRegressor(n_estimators=10000, early_stopping_rounds=50, **params)\n",
    "\n",
    "regressor.fit(X_train, y_train,\n",
    "              eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "              verbose=False)\n",
    "\n",
    "y_pred = regressor.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Stage 2 ==============================')\n",
    "print(f'best score = {weighted_absolute_percentage_error(y_valid, y_pred):.2%}')\n",
    "print('boosting params ---------------------------')\n",
    "print(f'fixed learning rate: {params['learning_rate']}')\n",
    "print(f'best boosting round: {regressor.best_iteration}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = xgb.XGBRegressor(n_estimators=regressor.best_iteration, **params)\n",
    "        \n",
    "regressor.fit(X_train, y_train,\n",
    "                    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame(data=regressor.feature_importances_,\n",
    "                                  index=regressor.feature_names_in_,\n",
    "                                  columns=['importance'])\n",
    "feature_importance.sort_values('importance').plot.barh(title='Feature Importance', legend=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_result = regressor.evals_result()\n",
    "\n",
    "rmse_train_loss = loss_result['validation_0']['rmse']\n",
    "rmse_test_loss = loss_result['validation_1']['rmse']\n",
    "\n",
    "wape_train_loss = loss_result['validation_0']['weighted_absolute_percentage_error']\n",
    "wape_test_loss = loss_result['validation_1']['weighted_absolute_percentage_error']\n",
    "\n",
    "estimators = range(len(rmse_train_loss))\n",
    "\n",
    "# fig, ax1 = plt.subplots(figsize=(16, 9))\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 9))\n",
    "\n",
    "ax1.plot(estimators, rmse_train_loss, label='Train Loss')\n",
    "ax1.plot(estimators, rmse_test_loss, label='Test Loss')\n",
    "ax1.set_title('RMSE Loss')\n",
    "ax1.legend(loc='best')\n",
    "\n",
    "ax2.plot(estimators, wape_train_loss, label='Train Loss')\n",
    "ax2.plot(estimators, wape_test_loss, label='Test Loss')\n",
    "ax2.set_title('WAPE Loss')\n",
    "ax2.legend(loc='best')\n",
    "\n",
    "fig.supxlabel('Estimators')\n",
    "fig.supylabel('Loss')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({'business_date': test['business_date'],\n",
    "                        'venue_xref_id': test['venue_xref_id'],\n",
    "                        'target': y_test, \n",
    "                        'prediction': y_pred})\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax1 = plt.subplots(figsize=(15, 10))\n",
    "# ax1.set_xlabel('Business Date')\n",
    "# ax1.set_ylabel('Sales Revenue')\n",
    "# ax1.scatter(pred_df['business_date'], pred_df['target'], color=COLORS[0])\n",
    "\n",
    "# ax2 = ax1.twinx()\n",
    "# ax2.scatter(pred_df['business_date'], pred_df['prediction'], color=COLORS[3])\n",
    "# ax2.set_axis_off()\n",
    "\n",
    "# fig.suptitle('Target vs Prediction by Date')\n",
    "# fig.legend(['target', 'prediction'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax1 = plt.subplots(figsize=(15, 10))\n",
    "# ax1.set_xlabel('Encoded Venue ID')\n",
    "# ax1.set_ylabel('Sales Revenue')\n",
    "# ax1.scatter(pred_df['venue_xref_id'], pred_df['target'], color=COLORS[0])\n",
    "\n",
    "# ax2 = ax1.twinx()\n",
    "# ax2.scatter(pred_df['venue_xref_id'], pred_df['prediction'], color=COLORS[3])\n",
    "# ax2.set_axis_off()\n",
    "\n",
    "# fig.suptitle('Target vs Prediction by Venue')\n",
    "# fig.legend(['target', 'prediction'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = regressor.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wape_train_score = weighted_absolute_percentage_error(y_train, train_pred)\n",
    "print(f'WAPE Score on Train set: {wape_train_score:.2%}')\n",
    "\n",
    "wape_test_score = weighted_absolute_percentage_error(pred_df['target'], pred_df['prediction'])\n",
    "print(f'WAPE Score on Test set: {wape_test_score:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_train_score = root_mean_squared_error(y_train, train_pred)\n",
    "print(f'RMSE Score on Train set: {rmse_train_score:.2f}')\n",
    "\n",
    "rmse_test_score = root_mean_squared_error(pred_df['target'], pred_df['prediction'])\n",
    "print(f'RMSE Score on Test set: {rmse_test_score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_train_score = mean_absolute_error(y_train, train_pred)\n",
    "print(f'MAE Score on Train set: {mae_train_score:.2f}')\n",
    "\n",
    "mae_test_score = mean_absolute_error(pred_df['target'], pred_df['prediction'])\n",
    "print(f'MAE Score on Test set: {mae_test_score:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df['error'] = np.abs(pred_df['target'] - pred_df['prediction'])\n",
    "\n",
    "print(f\"Best 10 Predictions:\\n{pred_df.sort_values('error', ascending=True).head(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Worst 10 Predictions:\\n{pred_df.sort_values('error', ascending=False).head(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain on All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = df[FEATURES]\n",
    "y_all = df[TARGET]\n",
    "\n",
    "X_all.shape, y_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = xgb.XGBRegressor(n_estimators=regressor.best_iteration, **params)\n",
    "        \n",
    "final_model.fit(X_all, y_all,\n",
    "                eval_set=[(X_all, y_all)],\n",
    "                verbose=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.save_model('CxC_TouchBistro_Forecaster.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cxc_touchbistro_challenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
